{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 1, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification (Prepare)\n",
    "\n",
    "Today's guided module project will be different. You already know how to do classification. You ready know how to extract features from documents. So? That means you're ready to combine and practice those skills in a kaggle competition. We we will open with a five minute sprint explaining the competition, and then give you 25 minutes to work. After those twenty five minutes are up, I will give a 5-minute demo an NLP technique that will help you with document classification (*and **maybe** the competition*).\n",
    "\n",
    "Today's all about having fun and practicing your skills. The competition will begin\n",
    "\n",
    "## Learning Objectives\n",
    "* <a href=\"#p0\">Part 0</a>: Kaggle Competition\n",
    "* <a href=\"#p1\">Part 1</a>: Text Feature Extraction & Classification Pipelines\n",
    "* <a href=\"#p2\">Part 2</a>: Latent Semantic Indexing\n",
    "* <a href=\"#p3\">Part 3</a>: Word Embeddings with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction & Classification Pipelines (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Sklearn pipelines allow you to stitch together multiple components of a machine learning process. The idea is that you can pass you raw data and get predictions out of the pipeline. This ability to pass raw input and receive a prediction from a singular class makes pipelines well suited for production, because you can pickle a a pipeline without worry about other data preprocessing steps. \n",
    "\n",
    "*Note:* Each time we call the pipeline during grid search, each component is fit again. The vectorizer (tf-idf) is transforming our entire vocabulary during each cross-validation fold. That transformation adds significant run time to our grid search. There *might* be interactions between the vectorizer and our classifier, so we estimate their performance together in the code below. However, if your goal is to reduce run time. Train your vectorizer separately (ie out of the grid-searched pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism',\n",
    "              'talk.religion.misc']\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline Components\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline\n",
    "pipe = Pipeline([\n",
    "                 #Vectorizer\n",
    "                 ('vect', vect),\n",
    "                 # Classifier\n",
    "                 ('clf', rfc)\n",
    "                ])\n",
    "\n",
    "# The pipeline puts together a bunch fit then transform,fit then predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': ( 0.75, 1.0),\n",
    "    'vect__min_df': (.02, .05),\n",
    "    'vect__max_features': (500,1000),\n",
    "    'clf__n_estimators':(5, 10,),\n",
    "    'clf__max_depth':(15,20)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-99e6964859a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-37fea0679188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Send me lots of money now'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you won the lottery in Nigeria'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search.predict(['Send me lots of money now', 'you won the lottery in Nigeria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along \n",
    "\n",
    "What you should be doing now:\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model (try using the pipe method I just demoed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1321</td>\n",
       "      <td>\\nSometimes, when whisky is batched, a few lef...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3861</td>\n",
       "      <td>\\nAn uncommon exclusive bottling of a 6 year o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>655</td>\n",
       "      <td>\\nThis release is a port version of Amrut’s In...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>555</td>\n",
       "      <td>\\nThis 41 year old single cask was aged in a s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1965</td>\n",
       "      <td>\\nQuite herbal on the nose, with aromas of dri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                        description  ratingCategory\n",
       "0  1321  \\nSometimes, when whisky is batched, a few lef...               1\n",
       "1  3861  \\nAn uncommon exclusive bottling of a 6 year o...               0\n",
       "2   655  \\nThis release is a port version of Amrut’s In...               1\n",
       "3   555  \\nThis 41 year old single cask was aged in a s...               1\n",
       "4  1965  \\nQuite herbal on the nose, with aromas of dri...               1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "clf = XGBClassifier()\n",
    "\n",
    "pipe = Pipeline([('vect', vect), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Search Space\n",
    "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 250 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 10.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4eb655d8b43c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ratingCategory\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': [x/100. for x in range(75, 101)],\n",
    "    'vect__min_df': [x/100. for x in range(2, 6)],\n",
    "    'clf__max_depth': range(5, 26, 5),\n",
    "    'clf__learning_rate': [x/100. for x in range(5, 36)],\n",
    "    'clf__min_child_weight': range(1, 36, 5),\n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(pipe, parameters, cv=4, n_jobs=-1, verbose=1, n_iter=250)\n",
    "grid_search.fit(train[\"description\"], train[\"ratingCategory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission File\n",
    "*Note:* In a typical Kaggle competition, you are only allowed two submissions a day, so you only submit if you feel you cannot achieve higher test accuracy. For this compeition the max daily submissions are capped at **20**. Submit for each demo and for your assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test sample\n",
    "pred = grid_search.predict(test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test['id'], 'category':pred})\n",
    "submission['category'] = submission['category'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Sure the Category is an Integer\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "submission.to_csv('./data/submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You're trying to achieve 90% Accuracy on your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, # Just here for demo. \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \n",
    "    'lsi__svd__n_components': [80, 100, 120],\n",
    "    'lsi__vect__max_df':[.9, .95, 1.0],\n",
    "    'clf__n_estimators': range(80, 141, 20),\n",
    "    'clf__max_depth': range(5, 26, 5),\n",
    "    'clf__learning_rate': [x/100. for x in range(20, 36)],\n",
    "    'clf__min_child_weight': range(1, 6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])\n",
    "\n",
    "\n",
    "# Pipe\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('lsi', Pipeline(memory=None,\n",
      "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), norm=...\n",
      "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
      "       subsample=1, verbosity=1))])\n"
     ]
    }
   ],
   "source": [
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 200 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 19.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-fda22e0cca35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "grid_search = RandomizedSearchCV(pipe,params, cv=4, n_jobs=-1, verbose=1, n_iter=200)\n",
    "grid_search.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test sample\n",
    "pred = grid_search.predict(test['description'])\n",
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')\n",
    "\n",
    "# Make Sure the Category is an Integer\n",
    "submission.head()\n",
    "\n",
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "submission.to_csv('./data/submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model & try: \n",
    "    - Creating a Text Extraction & Classification Pipeline\n",
    "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
    "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
    "4. Make a submission to Kaggle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = ...\n",
    "vect = ...\n",
    "clf = ...\n",
    "\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Your Search Space\n",
    "You're looking for both the best hyperparameters of your vectorizer and your classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'lsi__svd__n_components': [10,100,250],\n",
    "    'vect__max_df': (0.75, 1.0),\n",
    "    'clf__max_depth':(5,10,15,20)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=4, verbose=1)\n",
    "grid_search.fit(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Submission File\n",
    "*Note:* You are only allowed two submissions a day. Only submit if you feel you cannot achieve higher test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test sample\n",
    "pred = grid_search.predict(test['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test['id'], 'category':pred})\n",
    "submission['category'] = submission['category'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Sure the Category is an Integer\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subNumber = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "\n",
    "submission.to_csv(f'./data/submission{subNumber}.csv', index=False)\n",
    "subNumber += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Continue to apply Latent Semantic Indexing (LSI) to various datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings with Spacy (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Two bananas in pyjamas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "bananas_vector = doc.vector\n",
    "print(len(bananas_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(docs):\n",
    "    return [nlp(doc).vector for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = get_word_vectors(data.data)\n",
    "\n",
    "len(X) == len(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cowley/venv/U4-S1-NLP/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>ratingCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nSometimes, when whisky is batched, a few lef...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nAn uncommon exclusive bottling of a 6 year o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nThis release is a port version of Amrut’s In...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThis 41 year old single cask was aged in a s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nQuite herbal on the nose, with aromas of dri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  ratingCategory\n",
       "0  \\nSometimes, when whisky is batched, a few lef...               1\n",
       "1  \\nAn uncommon exclusive bottling of a 6 year o...               0\n",
       "2  \\nThis release is a port version of Amrut’s In...               1\n",
       "3  \\nThis 41 year old single cask was aged in a s...               1\n",
       "4  \\nQuite herbal on the nose, with aromas of dri...               1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"description\", \"ratingCategory\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.67550606e-02,  2.17486411e-01, -1.39248788e-01, -8.63811150e-02,\n",
       "        1.08062603e-01,  8.21812302e-02, -6.99262992e-02, -7.88967833e-02,\n",
       "       -4.63048331e-02,  1.67534256e+00, -8.29739422e-02,  8.47098455e-02,\n",
       "       -1.61168594e-02, -1.15673296e-01, -1.00931458e-01, -1.48674801e-01,\n",
       "       -2.86327489e-02,  1.00717258e+00, -8.84611458e-02, -3.00116055e-02,\n",
       "       -2.12408844e-02, -1.98691525e-03, -2.18204875e-02, -2.43686680e-02,\n",
       "       -8.50067884e-02, -5.28534129e-02, -3.88755389e-02,  2.85331737e-02,\n",
       "        9.71467644e-02, -1.52220830e-01, -7.67232990e-03,  3.68110649e-02,\n",
       "        3.03680412e-02, -3.20375822e-02,  4.51759845e-02, -2.22320929e-02,\n",
       "        2.99556088e-02, -3.53264883e-02, -8.09046626e-02,  4.02572937e-02,\n",
       "       -4.52380776e-02,  4.00653407e-02,  1.01217575e-01, -1.02363355e-01,\n",
       "        6.43703416e-02,  1.21800080e-01, -8.15031528e-02, -7.51190484e-02,\n",
       "        4.85178418e-02,  3.61067131e-02, -5.62809184e-02,  4.18828800e-02,\n",
       "       -9.56438202e-03, -1.31019577e-02,  6.11267164e-02, -5.16553111e-02,\n",
       "        3.51855718e-03, -5.93302697e-02,  1.33471452e-02, -5.46418168e-02,\n",
       "       -6.40768744e-03, -6.65486455e-02,  3.20854299e-02,  1.58353090e-01,\n",
       "       -7.78233483e-02, -9.54134837e-02,  3.75243835e-02,  3.79275940e-02,\n",
       "       -3.24006788e-02,  2.41587281e-01,  2.67855637e-02,  1.46980003e-01,\n",
       "        9.65503827e-02,  6.04135841e-02,  5.40188625e-02,  6.36219755e-02,\n",
       "        5.70697784e-02, -1.55114708e-02, -6.78861961e-02,  7.11889267e-02,\n",
       "       -4.57446128e-02,  1.15371859e-02, -6.99428171e-02,  3.00093442e-02,\n",
       "        1.31707115e-03, -2.86772579e-01,  3.16026181e-01,  1.96002334e-01,\n",
       "        1.60671636e-01,  4.46167700e-02,  5.54057732e-02, -1.77956037e-02,\n",
       "        1.57796994e-01,  3.04826107e-02,  1.18372798e-01,  3.78044136e-02,\n",
       "       -5.00417836e-02,  3.89910862e-02, -1.63217619e-01, -8.58448744e-02,\n",
       "        5.98485991e-02, -1.09544275e-02, -1.42839298e-01, -8.65477789e-03,\n",
       "       -2.82965209e-02, -6.20324790e-01,  2.65786629e-02, -1.53543660e-02,\n",
       "        2.16272488e-01,  2.55124606e-02, -3.01614776e-02, -1.43356591e-01,\n",
       "       -3.10345758e-02, -1.31078035e-01, -1.50374129e-01,  7.50435283e-03,\n",
       "        8.50384533e-02,  3.82597856e-02,  5.52595966e-02, -2.99246181e-02,\n",
       "        1.69796068e-02,  7.74004087e-02, -5.90990856e-02,  2.27499288e-02,\n",
       "       -5.20684943e-02,  8.32726732e-02,  7.28703439e-02, -7.48897195e-02,\n",
       "        5.74704297e-02, -3.18312570e-02,  1.59947369e-02, -8.15742239e-02,\n",
       "       -4.17144038e-02,  4.75331731e-02,  4.86046486e-02, -3.34403850e-03,\n",
       "        1.82459522e-02, -7.70179108e-02,  3.96432839e-02,  3.10516804e-02,\n",
       "       -1.03874719e+00,  1.90209791e-01,  1.62073895e-01,  1.03557028e-03,\n",
       "       -6.16472252e-02,  2.24926975e-02, -4.34726365e-02, -9.52140708e-03,\n",
       "        1.34421259e-01, -1.82343200e-02,  9.19859260e-02, -1.61238778e-02,\n",
       "        2.28434935e-01,  1.18926816e-01, -1.44609630e-01,  1.96701335e-03,\n",
       "       -9.02838260e-02, -6.16818592e-02, -1.84267424e-02,  8.16498548e-02,\n",
       "       -3.21386419e-02,  5.38884476e-02,  1.53115476e-02,  7.37509876e-02,\n",
       "       -1.06026091e-01, -1.16821699e-01, -2.18026191e-02, -1.10028177e-01,\n",
       "        8.91874656e-02,  1.50409834e-02,  7.72751868e-02,  1.25687540e-01,\n",
       "       -3.22907604e-02,  5.39286546e-02, -2.01050118e-01, -1.38029326e-02,\n",
       "       -8.34357813e-02, -9.00421664e-02,  8.96297097e-02, -1.48434430e-01,\n",
       "       -5.84306903e-02, -9.70491320e-02, -8.20585713e-03, -2.99466960e-02,\n",
       "        2.23509595e-03, -7.90550262e-02, -3.73679511e-02, -4.46197316e-02,\n",
       "        1.82006229e-02, -5.76254725e-02,  1.99167267e-03,  7.18337297e-02,\n",
       "        2.41990685e-02,  1.58399511e-02, -2.18793768e-02, -9.89985932e-03,\n",
       "        6.40814528e-02, -6.13718815e-02, -1.11674048e-01,  1.00324705e-01,\n",
       "       -1.00514688e-01, -9.24723148e-02, -7.25114122e-02, -1.28334416e-02,\n",
       "        1.66479394e-01,  8.65908712e-02,  2.51513086e-02,  2.05176771e-02,\n",
       "        9.89832655e-02,  3.87072854e-04, -1.31764382e-01, -2.33728327e-02,\n",
       "       -2.69102249e-02, -1.34848595e-01,  2.65630651e-02,  1.38402313e-01,\n",
       "       -1.66778490e-02,  1.03491629e-02, -4.97255474e-02,  7.57269934e-02,\n",
       "       -7.52002597e-02,  6.71452209e-02,  5.92398904e-02,  7.70933703e-02,\n",
       "        7.30587840e-02,  2.27728654e-02, -3.49727310e-02,  1.26141772e-01,\n",
       "        6.36828598e-03,  1.28286108e-01,  1.20686954e-02,  2.13961676e-02,\n",
       "        3.84524949e-02,  9.48235393e-02, -6.86500072e-02, -7.02346861e-02,\n",
       "       -6.99323267e-02, -1.58278033e-01,  5.16961403e-02,  4.71388437e-02,\n",
       "       -3.43012325e-02,  4.83978242e-02, -1.69183329e-01, -4.53796536e-02,\n",
       "        3.04854233e-02,  4.88160513e-02, -1.24649048e-01, -1.04970165e-01,\n",
       "       -8.64592418e-02,  5.51374108e-02,  7.81922042e-02, -2.92669926e-02,\n",
       "       -5.86480536e-02,  9.89177148e-04,  3.60662453e-02,  1.46576330e-01,\n",
       "        1.61460176e-01, -9.15234834e-02, -2.57627405e-02,  1.55300692e-01,\n",
       "        1.00190796e-01,  2.20081490e-02, -9.60066263e-03,  1.36905193e-01,\n",
       "       -3.22607867e-02, -2.12020967e-02, -6.17986731e-02, -6.14032932e-02,\n",
       "        2.10585240e-02,  6.25049090e-03, -8.59537944e-02, -6.55349642e-02,\n",
       "        3.72211002e-02, -1.40615165e-01, -1.96337953e-01, -2.05468144e-02,\n",
       "        1.31405825e-02,  9.84849259e-02, -4.83715311e-02,  2.18721494e-01,\n",
       "        2.50574440e-01,  4.39682454e-02,  9.89086553e-02, -1.03148304e-01,\n",
       "       -1.47065492e-02,  5.81453554e-02,  1.64416671e-01, -8.61592144e-02,\n",
       "        1.54656187e-01, -4.58907560e-02, -1.63037866e-01,  1.15813101e-02,\n",
       "       -3.74428593e-02,  1.42398439e-02, -1.43216491e-01,  1.80973131e-02,\n",
       "        7.47582933e-04, -2.09036931e-01, -7.23463818e-02,  4.28469256e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(train[\"description\"][0]).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4087, included: 3654\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "class MyVectorizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__counter = Counter()\n",
    "        \n",
    "    def __clean(self, s):\n",
    "        doc = tokenizer(s)\n",
    "        doc_tokens = [\n",
    "            re.sub(r\"[^a-z0-9]\", \"\", t.lemma_.lower()).strip() for t in doc\n",
    "            if not t.is_stop and not t.is_punct and t.text.strip()\n",
    "        ]\n",
    "        return \" \".join(doc_tokens)\n",
    "\n",
    "    \n",
    "    def fit_transform(self, docs):\n",
    "        self.fit(docs)\n",
    "        return self.transform(docs)\n",
    "    \n",
    "    \n",
    "    def fit(self, docs):\n",
    "        cleaned = [self.__clean(s) for s in docs]\n",
    "        \n",
    "        self.__counter = Counter()\n",
    "        [self.__counter.update(set(s.split(\" \"))) for s in cleaned]\n",
    "    \n",
    "    \n",
    "    def transform(self, docs):\n",
    "        cleaned = [self.__clean(s) for s in docs]\n",
    "        total = len(cleaned)\n",
    "        word_to_pct = dict()\n",
    "        for word, count in  self.__counter.items():\n",
    "            pct = count/total\n",
    "            if 0.001 <= pct and pct <= 0.95:\n",
    "                word_to_pct[word] = pct\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        cleaned2 = [\n",
    "            \" \".join(w for w in s.split(\" \") if w in word_to_pct)\n",
    "            for s in cleaned\n",
    "        ]\n",
    "\n",
    "        return [nlp(s).vector for s in cleaned2]\n",
    "\n",
    "vectorizer = MyVectorizer()\n",
    "X_train = pd.DataFrame(vectorizer.fit_transform(train[\"description\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.028498</td>\n",
       "      <td>0.188128</td>\n",
       "      <td>-0.138604</td>\n",
       "      <td>-0.122122</td>\n",
       "      <td>0.177986</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.103738</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>-0.013870</td>\n",
       "      <td>1.248903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128297</td>\n",
       "      <td>0.021213</td>\n",
       "      <td>-0.064529</td>\n",
       "      <td>-0.045900</td>\n",
       "      <td>-0.143962</td>\n",
       "      <td>0.008417</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>-0.304484</td>\n",
       "      <td>-0.020319</td>\n",
       "      <td>0.040548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015520</td>\n",
       "      <td>0.189974</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>-0.064801</td>\n",
       "      <td>-0.077039</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>-0.072463</td>\n",
       "      <td>1.150055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133380</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>-0.076733</td>\n",
       "      <td>-0.170855</td>\n",
       "      <td>-0.103147</td>\n",
       "      <td>0.011155</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>-0.207345</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.089354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120923</td>\n",
       "      <td>0.126532</td>\n",
       "      <td>-0.131240</td>\n",
       "      <td>-0.040462</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>-0.160508</td>\n",
       "      <td>-0.061130</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>1.068527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>0.021846</td>\n",
       "      <td>-0.084266</td>\n",
       "      <td>-0.166037</td>\n",
       "      <td>-0.094324</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.107538</td>\n",
       "      <td>-0.234552</td>\n",
       "      <td>-0.162804</td>\n",
       "      <td>0.066963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.096525</td>\n",
       "      <td>0.210867</td>\n",
       "      <td>-0.070901</td>\n",
       "      <td>-0.088450</td>\n",
       "      <td>-0.008463</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>-0.179191</td>\n",
       "      <td>0.086961</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>1.134071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044764</td>\n",
       "      <td>0.089584</td>\n",
       "      <td>-0.131383</td>\n",
       "      <td>-0.172626</td>\n",
       "      <td>-0.203195</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>-0.393445</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>0.109669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.136679</td>\n",
       "      <td>0.232677</td>\n",
       "      <td>-0.210757</td>\n",
       "      <td>-0.048334</td>\n",
       "      <td>-0.094839</td>\n",
       "      <td>0.025923</td>\n",
       "      <td>-0.332620</td>\n",
       "      <td>0.095476</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019904</td>\n",
       "      <td>0.148655</td>\n",
       "      <td>-0.159560</td>\n",
       "      <td>-0.353477</td>\n",
       "      <td>-0.343024</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.193477</td>\n",
       "      <td>-0.583076</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>0.137693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.154905</td>\n",
       "      <td>0.221724</td>\n",
       "      <td>-0.137159</td>\n",
       "      <td>-0.160858</td>\n",
       "      <td>0.104132</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>-0.051697</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.082452</td>\n",
       "      <td>1.294517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130995</td>\n",
       "      <td>0.140425</td>\n",
       "      <td>-0.163183</td>\n",
       "      <td>-0.140591</td>\n",
       "      <td>-0.114801</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>0.051055</td>\n",
       "      <td>-0.394038</td>\n",
       "      <td>-0.032219</td>\n",
       "      <td>0.166105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.030661</td>\n",
       "      <td>-0.060032</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>-0.105039</td>\n",
       "      <td>-0.050725</td>\n",
       "      <td>-0.034064</td>\n",
       "      <td>-0.040815</td>\n",
       "      <td>1.267221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106803</td>\n",
       "      <td>0.148391</td>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.148248</td>\n",
       "      <td>-0.191404</td>\n",
       "      <td>-0.006327</td>\n",
       "      <td>-0.006516</td>\n",
       "      <td>-0.138992</td>\n",
       "      <td>-0.035500</td>\n",
       "      <td>0.142647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.095956</td>\n",
       "      <td>0.231727</td>\n",
       "      <td>-0.058846</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>-0.025759</td>\n",
       "      <td>-0.075900</td>\n",
       "      <td>-0.088808</td>\n",
       "      <td>0.114123</td>\n",
       "      <td>-0.018964</td>\n",
       "      <td>0.990942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054586</td>\n",
       "      <td>0.177348</td>\n",
       "      <td>-0.138888</td>\n",
       "      <td>-0.229308</td>\n",
       "      <td>-0.272857</td>\n",
       "      <td>0.021449</td>\n",
       "      <td>0.174141</td>\n",
       "      <td>-0.327706</td>\n",
       "      <td>-0.045108</td>\n",
       "      <td>0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.155937</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>-0.092533</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>-0.062582</td>\n",
       "      <td>-0.132807</td>\n",
       "      <td>0.059727</td>\n",
       "      <td>0.080933</td>\n",
       "      <td>1.462363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105423</td>\n",
       "      <td>0.204754</td>\n",
       "      <td>-0.185349</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.092981</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.135142</td>\n",
       "      <td>-0.306221</td>\n",
       "      <td>-0.055026</td>\n",
       "      <td>0.179862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.020858</td>\n",
       "      <td>0.184540</td>\n",
       "      <td>-0.097321</td>\n",
       "      <td>-0.116242</td>\n",
       "      <td>0.032797</td>\n",
       "      <td>-0.017879</td>\n",
       "      <td>-0.060531</td>\n",
       "      <td>0.059191</td>\n",
       "      <td>-0.040205</td>\n",
       "      <td>0.970501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103237</td>\n",
       "      <td>0.167464</td>\n",
       "      <td>-0.185512</td>\n",
       "      <td>-0.222649</td>\n",
       "      <td>-0.159381</td>\n",
       "      <td>0.071477</td>\n",
       "      <td>0.134108</td>\n",
       "      <td>-0.369714</td>\n",
       "      <td>-0.035182</td>\n",
       "      <td>0.117496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.132330</td>\n",
       "      <td>0.114758</td>\n",
       "      <td>-0.076041</td>\n",
       "      <td>-0.008581</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.014918</td>\n",
       "      <td>-0.085192</td>\n",
       "      <td>0.086859</td>\n",
       "      <td>-0.055604</td>\n",
       "      <td>0.840614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031452</td>\n",
       "      <td>0.138783</td>\n",
       "      <td>-0.138595</td>\n",
       "      <td>-0.225609</td>\n",
       "      <td>-0.207761</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>0.081699</td>\n",
       "      <td>-0.489181</td>\n",
       "      <td>-0.091382</td>\n",
       "      <td>0.105379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.054847</td>\n",
       "      <td>0.127634</td>\n",
       "      <td>-0.146601</td>\n",
       "      <td>-0.035688</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>-0.035883</td>\n",
       "      <td>-0.096391</td>\n",
       "      <td>0.136893</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>1.401130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092053</td>\n",
       "      <td>0.127724</td>\n",
       "      <td>-0.128296</td>\n",
       "      <td>-0.100409</td>\n",
       "      <td>-0.186926</td>\n",
       "      <td>-0.023322</td>\n",
       "      <td>0.070295</td>\n",
       "      <td>-0.262135</td>\n",
       "      <td>-0.009515</td>\n",
       "      <td>0.020161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.111966</td>\n",
       "      <td>0.224461</td>\n",
       "      <td>-0.052194</td>\n",
       "      <td>-0.102919</td>\n",
       "      <td>0.026827</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>-0.104594</td>\n",
       "      <td>0.103151</td>\n",
       "      <td>-0.064311</td>\n",
       "      <td>1.136654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165285</td>\n",
       "      <td>0.163974</td>\n",
       "      <td>-0.167586</td>\n",
       "      <td>-0.126193</td>\n",
       "      <td>-0.242601</td>\n",
       "      <td>0.097002</td>\n",
       "      <td>0.152910</td>\n",
       "      <td>-0.345077</td>\n",
       "      <td>0.068697</td>\n",
       "      <td>0.179329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.049796</td>\n",
       "      <td>0.092660</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.080605</td>\n",
       "      <td>-0.121469</td>\n",
       "      <td>0.066160</td>\n",
       "      <td>-0.251814</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>1.010025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114285</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>-0.098086</td>\n",
       "      <td>-0.234647</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.034222</td>\n",
       "      <td>0.097808</td>\n",
       "      <td>-0.409343</td>\n",
       "      <td>-0.050235</td>\n",
       "      <td>0.216690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.054295</td>\n",
       "      <td>0.150484</td>\n",
       "      <td>-0.094874</td>\n",
       "      <td>-0.144877</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>0.019258</td>\n",
       "      <td>-0.046008</td>\n",
       "      <td>0.060910</td>\n",
       "      <td>-0.040174</td>\n",
       "      <td>0.873364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052250</td>\n",
       "      <td>0.207983</td>\n",
       "      <td>-0.089124</td>\n",
       "      <td>-0.193080</td>\n",
       "      <td>-0.179295</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.149830</td>\n",
       "      <td>-0.312695</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.020276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.058881</td>\n",
       "      <td>0.163347</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.073494</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>-0.059065</td>\n",
       "      <td>-0.001380</td>\n",
       "      <td>-0.008788</td>\n",
       "      <td>0.656395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041868</td>\n",
       "      <td>0.237362</td>\n",
       "      <td>-0.196532</td>\n",
       "      <td>-0.234726</td>\n",
       "      <td>-0.198414</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.239764</td>\n",
       "      <td>-0.464825</td>\n",
       "      <td>-0.095042</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.100399</td>\n",
       "      <td>0.079897</td>\n",
       "      <td>-0.007848</td>\n",
       "      <td>-0.035037</td>\n",
       "      <td>0.073051</td>\n",
       "      <td>-0.010864</td>\n",
       "      <td>-0.076765</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.059279</td>\n",
       "      <td>1.167828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013186</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>-0.148842</td>\n",
       "      <td>-0.212552</td>\n",
       "      <td>-0.203252</td>\n",
       "      <td>-0.014746</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.031365</td>\n",
       "      <td>0.149680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.058117</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>-0.042705</td>\n",
       "      <td>-0.063315</td>\n",
       "      <td>0.040747</td>\n",
       "      <td>0.038924</td>\n",
       "      <td>-0.189689</td>\n",
       "      <td>0.079524</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>1.280461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031627</td>\n",
       "      <td>0.105489</td>\n",
       "      <td>-0.219262</td>\n",
       "      <td>-0.058634</td>\n",
       "      <td>-0.180212</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.040696</td>\n",
       "      <td>-0.594045</td>\n",
       "      <td>-0.086513</td>\n",
       "      <td>0.179421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.056632</td>\n",
       "      <td>0.205568</td>\n",
       "      <td>-0.188050</td>\n",
       "      <td>-0.067825</td>\n",
       "      <td>-0.029115</td>\n",
       "      <td>-0.066592</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>0.177288</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.587956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090283</td>\n",
       "      <td>0.112017</td>\n",
       "      <td>-0.261069</td>\n",
       "      <td>-0.212723</td>\n",
       "      <td>-0.202431</td>\n",
       "      <td>0.034773</td>\n",
       "      <td>0.173556</td>\n",
       "      <td>-0.382630</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>0.086404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.079891</td>\n",
       "      <td>0.160634</td>\n",
       "      <td>-0.084137</td>\n",
       "      <td>-0.090037</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>-0.085887</td>\n",
       "      <td>-0.107504</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>1.307971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154526</td>\n",
       "      <td>0.093217</td>\n",
       "      <td>-0.076030</td>\n",
       "      <td>-0.087580</td>\n",
       "      <td>-0.194103</td>\n",
       "      <td>-0.073930</td>\n",
       "      <td>-0.015896</td>\n",
       "      <td>-0.222053</td>\n",
       "      <td>0.043591</td>\n",
       "      <td>0.117330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.028498  0.188128 -0.138604 -0.122122  0.177986 -0.026991 -0.103738   \n",
       "1  -0.015520  0.189974 -0.016040  0.006239  0.001201 -0.064801 -0.077039   \n",
       "2   0.120923  0.126532 -0.131240 -0.040462  0.074998 -0.160508 -0.061130   \n",
       "3  -0.096525  0.210867 -0.070901 -0.088450 -0.008463 -0.064484 -0.179191   \n",
       "4  -0.136679  0.232677 -0.210757 -0.048334 -0.094839  0.025923 -0.332620   \n",
       "5  -0.154905  0.221724 -0.137159 -0.160858  0.104132  0.018887 -0.051697   \n",
       "6   0.088769  0.030661 -0.060032  0.026390  0.001968 -0.105039 -0.050725   \n",
       "7  -0.095956  0.231727 -0.058846 -0.070600 -0.025759 -0.075900 -0.088808   \n",
       "8  -0.155937  0.158490 -0.092533  0.013157  0.024642 -0.062582 -0.132807   \n",
       "9  -0.020858  0.184540 -0.097321 -0.116242  0.032797 -0.017879 -0.060531   \n",
       "10 -0.132330  0.114758 -0.076041 -0.008581  0.000611 -0.014918 -0.085192   \n",
       "11 -0.054847  0.127634 -0.146601 -0.035688  0.022153 -0.035883 -0.096391   \n",
       "12 -0.111966  0.224461 -0.052194 -0.102919  0.026827  0.074860 -0.104594   \n",
       "13 -0.049796  0.092660  0.003053  0.080605 -0.121469  0.066160 -0.251814   \n",
       "14 -0.054295  0.150484 -0.094874 -0.144877  0.084914  0.019258 -0.046008   \n",
       "15 -0.058881  0.163347 -0.108300 -0.000325 -0.073494 -0.026129 -0.059065   \n",
       "16 -0.100399  0.079897 -0.007848 -0.035037  0.073051 -0.010864 -0.076765   \n",
       "17 -0.058117  0.161704 -0.042705 -0.063315  0.040747  0.038924 -0.189689   \n",
       "18 -0.056632  0.205568 -0.188050 -0.067825 -0.029115 -0.066592 -0.127453   \n",
       "19 -0.079891  0.160634 -0.084137 -0.090037  0.051600 -0.085887 -0.107504   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0   0.041936 -0.013870  1.248903  ... -0.128297  0.021213 -0.064529 -0.045900   \n",
       "1   0.021943 -0.072463  1.150055  ... -0.133380  0.176411 -0.076733 -0.170855   \n",
       "2   0.095303  0.017329  1.068527  ... -0.040477  0.021846 -0.084266 -0.166037   \n",
       "3   0.086961  0.029514  1.134071  ... -0.044764  0.089584 -0.131383 -0.172626   \n",
       "4   0.095476  0.047830  0.785689  ...  0.019904  0.148655 -0.159560 -0.353477   \n",
       "5   0.015071 -0.082452  1.294517  ... -0.130995  0.140425 -0.163183 -0.140591   \n",
       "6  -0.034064 -0.040815  1.267221  ... -0.106803  0.148391 -0.044019 -0.148248   \n",
       "7   0.114123 -0.018964  0.990942  ... -0.054586  0.177348 -0.138888 -0.229308   \n",
       "8   0.059727  0.080933  1.462363  ... -0.105423  0.204754 -0.185349 -0.105309   \n",
       "9   0.059191 -0.040205  0.970501  ... -0.103237  0.167464 -0.185512 -0.222649   \n",
       "10  0.086859 -0.055604  0.840614  ... -0.031452  0.138783 -0.138595 -0.225609   \n",
       "11  0.136893 -0.002395  1.401130  ... -0.092053  0.127724 -0.128296 -0.100409   \n",
       "12  0.103151 -0.064311  1.136654  ... -0.165285  0.163974 -0.167586 -0.126193   \n",
       "13  0.023515  0.005768  1.010025  ... -0.114285  0.091071 -0.098086 -0.234647   \n",
       "14  0.060910 -0.040174  0.873364  ... -0.052250  0.207983 -0.089124 -0.193080   \n",
       "15 -0.001380 -0.008788  0.656395  ... -0.041868  0.237362 -0.196532 -0.234726   \n",
       "16  0.003783  0.059279  1.167828  ... -0.013186  0.092722 -0.148842 -0.212552   \n",
       "17  0.079524  0.019557  1.280461  ... -0.031627  0.105489 -0.219262 -0.058634   \n",
       "18  0.177288  0.002491  0.587956  ...  0.090283  0.112017 -0.261069 -0.212723   \n",
       "19  0.098706 -0.006845  1.307971  ... -0.154526  0.093217 -0.076030 -0.087580   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0  -0.143962  0.008417  0.034709 -0.304484 -0.020319  0.040548  \n",
       "1  -0.103147  0.011155  0.099502 -0.207345  0.005043  0.089354  \n",
       "2  -0.094324  0.001587  0.107538 -0.234552 -0.162804  0.066963  \n",
       "3  -0.203195  0.036219  0.135700 -0.393445 -0.038131  0.109669  \n",
       "4  -0.343024  0.007143  0.193477 -0.583076  0.031495  0.137693  \n",
       "5  -0.114801  0.072113  0.051055 -0.394038 -0.032219  0.166105  \n",
       "6  -0.191404 -0.006327 -0.006516 -0.138992 -0.035500  0.142647  \n",
       "7  -0.272857  0.021449  0.174141 -0.327706 -0.045108  0.006042  \n",
       "8  -0.092981  0.016343  0.135142 -0.306221 -0.055026  0.179862  \n",
       "9  -0.159381  0.071477  0.134108 -0.369714 -0.035182  0.117496  \n",
       "10 -0.207761  0.015069  0.081699 -0.489181 -0.091382  0.105379  \n",
       "11 -0.186926 -0.023322  0.070295 -0.262135 -0.009515  0.020161  \n",
       "12 -0.242601  0.097002  0.152910 -0.345077  0.068697  0.179329  \n",
       "13 -0.290015  0.034222  0.097808 -0.409343 -0.050235  0.216690  \n",
       "14 -0.179295  0.023917  0.149830 -0.312695  0.011964  0.020276  \n",
       "15 -0.198414  0.004398  0.239764 -0.464825 -0.095042  0.193000  \n",
       "16 -0.203252 -0.014746  0.134579 -0.464761 -0.031365  0.149680  \n",
       "17 -0.180212  0.017097  0.040696 -0.594045 -0.086513  0.179421  \n",
       "18 -0.202431  0.034773  0.173556 -0.382630 -0.017103  0.086404  \n",
       "19 -0.194103 -0.073930 -0.015896 -0.222053  0.043591  0.117330  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4087, included: 3654\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.028498</td>\n",
       "      <td>0.188128</td>\n",
       "      <td>-0.138604</td>\n",
       "      <td>-0.122122</td>\n",
       "      <td>0.177986</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.103738</td>\n",
       "      <td>0.041936</td>\n",
       "      <td>-0.013870</td>\n",
       "      <td>1.248903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128297</td>\n",
       "      <td>0.021213</td>\n",
       "      <td>-0.064529</td>\n",
       "      <td>-0.045900</td>\n",
       "      <td>-0.143962</td>\n",
       "      <td>0.008417</td>\n",
       "      <td>0.034709</td>\n",
       "      <td>-0.304484</td>\n",
       "      <td>-0.020319</td>\n",
       "      <td>0.040548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.015520</td>\n",
       "      <td>0.189974</td>\n",
       "      <td>-0.016040</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>-0.064801</td>\n",
       "      <td>-0.077039</td>\n",
       "      <td>0.021943</td>\n",
       "      <td>-0.072463</td>\n",
       "      <td>1.150055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133380</td>\n",
       "      <td>0.176411</td>\n",
       "      <td>-0.076733</td>\n",
       "      <td>-0.170855</td>\n",
       "      <td>-0.103147</td>\n",
       "      <td>0.011155</td>\n",
       "      <td>0.099502</td>\n",
       "      <td>-0.207345</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.089354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120923</td>\n",
       "      <td>0.126532</td>\n",
       "      <td>-0.131240</td>\n",
       "      <td>-0.040462</td>\n",
       "      <td>0.074998</td>\n",
       "      <td>-0.160508</td>\n",
       "      <td>-0.061130</td>\n",
       "      <td>0.095303</td>\n",
       "      <td>0.017329</td>\n",
       "      <td>1.068527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040477</td>\n",
       "      <td>0.021846</td>\n",
       "      <td>-0.084266</td>\n",
       "      <td>-0.166037</td>\n",
       "      <td>-0.094324</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.107538</td>\n",
       "      <td>-0.234552</td>\n",
       "      <td>-0.162804</td>\n",
       "      <td>0.066963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.096525</td>\n",
       "      <td>0.210867</td>\n",
       "      <td>-0.070901</td>\n",
       "      <td>-0.088450</td>\n",
       "      <td>-0.008463</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>-0.179191</td>\n",
       "      <td>0.086961</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>1.134071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044764</td>\n",
       "      <td>0.089584</td>\n",
       "      <td>-0.131383</td>\n",
       "      <td>-0.172626</td>\n",
       "      <td>-0.203195</td>\n",
       "      <td>0.036219</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>-0.393445</td>\n",
       "      <td>-0.038131</td>\n",
       "      <td>0.109669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.136679</td>\n",
       "      <td>0.232677</td>\n",
       "      <td>-0.210757</td>\n",
       "      <td>-0.048334</td>\n",
       "      <td>-0.094839</td>\n",
       "      <td>0.025923</td>\n",
       "      <td>-0.332620</td>\n",
       "      <td>0.095476</td>\n",
       "      <td>0.047830</td>\n",
       "      <td>0.785689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019904</td>\n",
       "      <td>0.148655</td>\n",
       "      <td>-0.159560</td>\n",
       "      <td>-0.353477</td>\n",
       "      <td>-0.343024</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.193477</td>\n",
       "      <td>-0.583076</td>\n",
       "      <td>0.031495</td>\n",
       "      <td>0.137693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.154905</td>\n",
       "      <td>0.221724</td>\n",
       "      <td>-0.137159</td>\n",
       "      <td>-0.160858</td>\n",
       "      <td>0.104132</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>-0.051697</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>-0.082452</td>\n",
       "      <td>1.294517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130995</td>\n",
       "      <td>0.140425</td>\n",
       "      <td>-0.163183</td>\n",
       "      <td>-0.140591</td>\n",
       "      <td>-0.114801</td>\n",
       "      <td>0.072113</td>\n",
       "      <td>0.051055</td>\n",
       "      <td>-0.394038</td>\n",
       "      <td>-0.032219</td>\n",
       "      <td>0.166105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.030661</td>\n",
       "      <td>-0.060032</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>-0.105039</td>\n",
       "      <td>-0.050725</td>\n",
       "      <td>-0.034064</td>\n",
       "      <td>-0.040815</td>\n",
       "      <td>1.267221</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106803</td>\n",
       "      <td>0.148391</td>\n",
       "      <td>-0.044019</td>\n",
       "      <td>-0.148248</td>\n",
       "      <td>-0.191404</td>\n",
       "      <td>-0.006327</td>\n",
       "      <td>-0.006516</td>\n",
       "      <td>-0.138992</td>\n",
       "      <td>-0.035500</td>\n",
       "      <td>0.142647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.095956</td>\n",
       "      <td>0.231727</td>\n",
       "      <td>-0.058846</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>-0.025759</td>\n",
       "      <td>-0.075900</td>\n",
       "      <td>-0.088808</td>\n",
       "      <td>0.114123</td>\n",
       "      <td>-0.018964</td>\n",
       "      <td>0.990942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054586</td>\n",
       "      <td>0.177348</td>\n",
       "      <td>-0.138888</td>\n",
       "      <td>-0.229308</td>\n",
       "      <td>-0.272857</td>\n",
       "      <td>0.021449</td>\n",
       "      <td>0.174141</td>\n",
       "      <td>-0.327706</td>\n",
       "      <td>-0.045108</td>\n",
       "      <td>0.006042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.155937</td>\n",
       "      <td>0.158490</td>\n",
       "      <td>-0.092533</td>\n",
       "      <td>0.013157</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>-0.062582</td>\n",
       "      <td>-0.132807</td>\n",
       "      <td>0.059727</td>\n",
       "      <td>0.080933</td>\n",
       "      <td>1.462363</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105423</td>\n",
       "      <td>0.204754</td>\n",
       "      <td>-0.185349</td>\n",
       "      <td>-0.105309</td>\n",
       "      <td>-0.092981</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>0.135142</td>\n",
       "      <td>-0.306221</td>\n",
       "      <td>-0.055026</td>\n",
       "      <td>0.179862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.020858</td>\n",
       "      <td>0.184540</td>\n",
       "      <td>-0.097321</td>\n",
       "      <td>-0.116242</td>\n",
       "      <td>0.032797</td>\n",
       "      <td>-0.017879</td>\n",
       "      <td>-0.060531</td>\n",
       "      <td>0.059191</td>\n",
       "      <td>-0.040205</td>\n",
       "      <td>0.970501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103237</td>\n",
       "      <td>0.167464</td>\n",
       "      <td>-0.185512</td>\n",
       "      <td>-0.222649</td>\n",
       "      <td>-0.159381</td>\n",
       "      <td>0.071477</td>\n",
       "      <td>0.134108</td>\n",
       "      <td>-0.369714</td>\n",
       "      <td>-0.035182</td>\n",
       "      <td>0.117496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.132330</td>\n",
       "      <td>0.114758</td>\n",
       "      <td>-0.076041</td>\n",
       "      <td>-0.008581</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>-0.014918</td>\n",
       "      <td>-0.085192</td>\n",
       "      <td>0.086859</td>\n",
       "      <td>-0.055604</td>\n",
       "      <td>0.840614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031452</td>\n",
       "      <td>0.138783</td>\n",
       "      <td>-0.138595</td>\n",
       "      <td>-0.225609</td>\n",
       "      <td>-0.207761</td>\n",
       "      <td>0.015069</td>\n",
       "      <td>0.081699</td>\n",
       "      <td>-0.489181</td>\n",
       "      <td>-0.091382</td>\n",
       "      <td>0.105379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.054847</td>\n",
       "      <td>0.127634</td>\n",
       "      <td>-0.146601</td>\n",
       "      <td>-0.035688</td>\n",
       "      <td>0.022153</td>\n",
       "      <td>-0.035883</td>\n",
       "      <td>-0.096391</td>\n",
       "      <td>0.136893</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>1.401130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.092053</td>\n",
       "      <td>0.127724</td>\n",
       "      <td>-0.128296</td>\n",
       "      <td>-0.100409</td>\n",
       "      <td>-0.186926</td>\n",
       "      <td>-0.023322</td>\n",
       "      <td>0.070295</td>\n",
       "      <td>-0.262135</td>\n",
       "      <td>-0.009515</td>\n",
       "      <td>0.020161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.111966</td>\n",
       "      <td>0.224461</td>\n",
       "      <td>-0.052194</td>\n",
       "      <td>-0.102919</td>\n",
       "      <td>0.026827</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>-0.104594</td>\n",
       "      <td>0.103151</td>\n",
       "      <td>-0.064311</td>\n",
       "      <td>1.136654</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165285</td>\n",
       "      <td>0.163974</td>\n",
       "      <td>-0.167586</td>\n",
       "      <td>-0.126193</td>\n",
       "      <td>-0.242601</td>\n",
       "      <td>0.097002</td>\n",
       "      <td>0.152910</td>\n",
       "      <td>-0.345077</td>\n",
       "      <td>0.068697</td>\n",
       "      <td>0.179329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.049796</td>\n",
       "      <td>0.092660</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.080605</td>\n",
       "      <td>-0.121469</td>\n",
       "      <td>0.066160</td>\n",
       "      <td>-0.251814</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>1.010025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114285</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>-0.098086</td>\n",
       "      <td>-0.234647</td>\n",
       "      <td>-0.290015</td>\n",
       "      <td>0.034222</td>\n",
       "      <td>0.097808</td>\n",
       "      <td>-0.409343</td>\n",
       "      <td>-0.050235</td>\n",
       "      <td>0.216690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.054295</td>\n",
       "      <td>0.150484</td>\n",
       "      <td>-0.094874</td>\n",
       "      <td>-0.144877</td>\n",
       "      <td>0.084914</td>\n",
       "      <td>0.019258</td>\n",
       "      <td>-0.046008</td>\n",
       "      <td>0.060910</td>\n",
       "      <td>-0.040174</td>\n",
       "      <td>0.873364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052250</td>\n",
       "      <td>0.207983</td>\n",
       "      <td>-0.089124</td>\n",
       "      <td>-0.193080</td>\n",
       "      <td>-0.179295</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.149830</td>\n",
       "      <td>-0.312695</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.020276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.058881</td>\n",
       "      <td>0.163347</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.073494</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>-0.059065</td>\n",
       "      <td>-0.001380</td>\n",
       "      <td>-0.008788</td>\n",
       "      <td>0.656395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041868</td>\n",
       "      <td>0.237362</td>\n",
       "      <td>-0.196532</td>\n",
       "      <td>-0.234726</td>\n",
       "      <td>-0.198414</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.239764</td>\n",
       "      <td>-0.464825</td>\n",
       "      <td>-0.095042</td>\n",
       "      <td>0.193000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.100399</td>\n",
       "      <td>0.079897</td>\n",
       "      <td>-0.007848</td>\n",
       "      <td>-0.035037</td>\n",
       "      <td>0.073051</td>\n",
       "      <td>-0.010864</td>\n",
       "      <td>-0.076765</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>0.059279</td>\n",
       "      <td>1.167828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013186</td>\n",
       "      <td>0.092722</td>\n",
       "      <td>-0.148842</td>\n",
       "      <td>-0.212552</td>\n",
       "      <td>-0.203252</td>\n",
       "      <td>-0.014746</td>\n",
       "      <td>0.134579</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.031365</td>\n",
       "      <td>0.149680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.058117</td>\n",
       "      <td>0.161704</td>\n",
       "      <td>-0.042705</td>\n",
       "      <td>-0.063315</td>\n",
       "      <td>0.040747</td>\n",
       "      <td>0.038924</td>\n",
       "      <td>-0.189689</td>\n",
       "      <td>0.079524</td>\n",
       "      <td>0.019557</td>\n",
       "      <td>1.280461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031627</td>\n",
       "      <td>0.105489</td>\n",
       "      <td>-0.219262</td>\n",
       "      <td>-0.058634</td>\n",
       "      <td>-0.180212</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.040696</td>\n",
       "      <td>-0.594045</td>\n",
       "      <td>-0.086513</td>\n",
       "      <td>0.179421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.056632</td>\n",
       "      <td>0.205568</td>\n",
       "      <td>-0.188050</td>\n",
       "      <td>-0.067825</td>\n",
       "      <td>-0.029115</td>\n",
       "      <td>-0.066592</td>\n",
       "      <td>-0.127453</td>\n",
       "      <td>0.177288</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.587956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090283</td>\n",
       "      <td>0.112017</td>\n",
       "      <td>-0.261069</td>\n",
       "      <td>-0.212723</td>\n",
       "      <td>-0.202431</td>\n",
       "      <td>0.034773</td>\n",
       "      <td>0.173556</td>\n",
       "      <td>-0.382630</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>0.086404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.079891</td>\n",
       "      <td>0.160634</td>\n",
       "      <td>-0.084137</td>\n",
       "      <td>-0.090037</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>-0.085887</td>\n",
       "      <td>-0.107504</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>-0.006845</td>\n",
       "      <td>1.307971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154526</td>\n",
       "      <td>0.093217</td>\n",
       "      <td>-0.076030</td>\n",
       "      <td>-0.087580</td>\n",
       "      <td>-0.194103</td>\n",
       "      <td>-0.073930</td>\n",
       "      <td>-0.015896</td>\n",
       "      <td>-0.222053</td>\n",
       "      <td>0.043591</td>\n",
       "      <td>0.117330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.028498  0.188128 -0.138604 -0.122122  0.177986 -0.026991 -0.103738   \n",
       "1  -0.015520  0.189974 -0.016040  0.006239  0.001201 -0.064801 -0.077039   \n",
       "2   0.120923  0.126532 -0.131240 -0.040462  0.074998 -0.160508 -0.061130   \n",
       "3  -0.096525  0.210867 -0.070901 -0.088450 -0.008463 -0.064484 -0.179191   \n",
       "4  -0.136679  0.232677 -0.210757 -0.048334 -0.094839  0.025923 -0.332620   \n",
       "5  -0.154905  0.221724 -0.137159 -0.160858  0.104132  0.018887 -0.051697   \n",
       "6   0.088769  0.030661 -0.060032  0.026390  0.001968 -0.105039 -0.050725   \n",
       "7  -0.095956  0.231727 -0.058846 -0.070600 -0.025759 -0.075900 -0.088808   \n",
       "8  -0.155937  0.158490 -0.092533  0.013157  0.024642 -0.062582 -0.132807   \n",
       "9  -0.020858  0.184540 -0.097321 -0.116242  0.032797 -0.017879 -0.060531   \n",
       "10 -0.132330  0.114758 -0.076041 -0.008581  0.000611 -0.014918 -0.085192   \n",
       "11 -0.054847  0.127634 -0.146601 -0.035688  0.022153 -0.035883 -0.096391   \n",
       "12 -0.111966  0.224461 -0.052194 -0.102919  0.026827  0.074860 -0.104594   \n",
       "13 -0.049796  0.092660  0.003053  0.080605 -0.121469  0.066160 -0.251814   \n",
       "14 -0.054295  0.150484 -0.094874 -0.144877  0.084914  0.019258 -0.046008   \n",
       "15 -0.058881  0.163347 -0.108300 -0.000325 -0.073494 -0.026129 -0.059065   \n",
       "16 -0.100399  0.079897 -0.007848 -0.035037  0.073051 -0.010864 -0.076765   \n",
       "17 -0.058117  0.161704 -0.042705 -0.063315  0.040747  0.038924 -0.189689   \n",
       "18 -0.056632  0.205568 -0.188050 -0.067825 -0.029115 -0.066592 -0.127453   \n",
       "19 -0.079891  0.160634 -0.084137 -0.090037  0.051600 -0.085887 -0.107504   \n",
       "\n",
       "         7         8         9    ...       290       291       292       293  \\\n",
       "0   0.041936 -0.013870  1.248903  ... -0.128297  0.021213 -0.064529 -0.045900   \n",
       "1   0.021943 -0.072463  1.150055  ... -0.133380  0.176411 -0.076733 -0.170855   \n",
       "2   0.095303  0.017329  1.068527  ... -0.040477  0.021846 -0.084266 -0.166037   \n",
       "3   0.086961  0.029514  1.134071  ... -0.044764  0.089584 -0.131383 -0.172626   \n",
       "4   0.095476  0.047830  0.785689  ...  0.019904  0.148655 -0.159560 -0.353477   \n",
       "5   0.015071 -0.082452  1.294517  ... -0.130995  0.140425 -0.163183 -0.140591   \n",
       "6  -0.034064 -0.040815  1.267221  ... -0.106803  0.148391 -0.044019 -0.148248   \n",
       "7   0.114123 -0.018964  0.990942  ... -0.054586  0.177348 -0.138888 -0.229308   \n",
       "8   0.059727  0.080933  1.462363  ... -0.105423  0.204754 -0.185349 -0.105309   \n",
       "9   0.059191 -0.040205  0.970501  ... -0.103237  0.167464 -0.185512 -0.222649   \n",
       "10  0.086859 -0.055604  0.840614  ... -0.031452  0.138783 -0.138595 -0.225609   \n",
       "11  0.136893 -0.002395  1.401130  ... -0.092053  0.127724 -0.128296 -0.100409   \n",
       "12  0.103151 -0.064311  1.136654  ... -0.165285  0.163974 -0.167586 -0.126193   \n",
       "13  0.023515  0.005768  1.010025  ... -0.114285  0.091071 -0.098086 -0.234647   \n",
       "14  0.060910 -0.040174  0.873364  ... -0.052250  0.207983 -0.089124 -0.193080   \n",
       "15 -0.001380 -0.008788  0.656395  ... -0.041868  0.237362 -0.196532 -0.234726   \n",
       "16  0.003783  0.059279  1.167828  ... -0.013186  0.092722 -0.148842 -0.212552   \n",
       "17  0.079524  0.019557  1.280461  ... -0.031627  0.105489 -0.219262 -0.058634   \n",
       "18  0.177288  0.002491  0.587956  ...  0.090283  0.112017 -0.261069 -0.212723   \n",
       "19  0.098706 -0.006845  1.307971  ... -0.154526  0.093217 -0.076030 -0.087580   \n",
       "\n",
       "         294       295       296       297       298       299  \n",
       "0  -0.143962  0.008417  0.034709 -0.304484 -0.020319  0.040548  \n",
       "1  -0.103147  0.011155  0.099502 -0.207345  0.005043  0.089354  \n",
       "2  -0.094324  0.001587  0.107538 -0.234552 -0.162804  0.066963  \n",
       "3  -0.203195  0.036219  0.135700 -0.393445 -0.038131  0.109669  \n",
       "4  -0.343024  0.007143  0.193477 -0.583076  0.031495  0.137693  \n",
       "5  -0.114801  0.072113  0.051055 -0.394038 -0.032219  0.166105  \n",
       "6  -0.191404 -0.006327 -0.006516 -0.138992 -0.035500  0.142647  \n",
       "7  -0.272857  0.021449  0.174141 -0.327706 -0.045108  0.006042  \n",
       "8  -0.092981  0.016343  0.135142 -0.306221 -0.055026  0.179862  \n",
       "9  -0.159381  0.071477  0.134108 -0.369714 -0.035182  0.117496  \n",
       "10 -0.207761  0.015069  0.081699 -0.489181 -0.091382  0.105379  \n",
       "11 -0.186926 -0.023322  0.070295 -0.262135 -0.009515  0.020161  \n",
       "12 -0.242601  0.097002  0.152910 -0.345077  0.068697  0.179329  \n",
       "13 -0.290015  0.034222  0.097808 -0.409343 -0.050235  0.216690  \n",
       "14 -0.179295  0.023917  0.149830 -0.312695  0.011964  0.020276  \n",
       "15 -0.198414  0.004398  0.239764 -0.464825 -0.095042  0.193000  \n",
       "16 -0.203252 -0.014746  0.134579 -0.464761 -0.031365  0.149680  \n",
       "17 -0.180212  0.017097  0.040696 -0.594045 -0.086513  0.179421  \n",
       "18 -0.202431  0.034773  0.173556 -0.382630 -0.017103  0.086404  \n",
       "19 -0.194103 -0.073930 -0.015896 -0.222053  0.043591  0.117330  \n",
       "\n",
       "[20 rows x 300 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectorizer.transform(train[\"description\"])).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.22,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=2, missing=None,\n",
       "       n_estimators=120, n_jobs=-1, nthread=None,\n",
       "       objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(n_jobs=-1, learning_rate=0.22, n_estimators=120, min_child_weight=2)\n",
    "xgb.fit(X_train, train[\"ratingCategory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 200 candidates, totalling 800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 17.4min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 27.8min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 31.1min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 34.1min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 37.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 40.7min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 48.2min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 52.3min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 55.9min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 60.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 64.4min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 69.1min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 73.4min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 78.6min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 83.5min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 88.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 93.5min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 99.3min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 104.8min\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    \"xgb__learning_rate\": [x/100. for x in range(18, 33)],\n",
    "    \"xgb__n_estimators\": range(80, 141, 20),\n",
    "    \"xgb__min_child_weight\": range(0, 5),\n",
    "    \"xgb__max_depth\": range(15, 31, 5),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(XGBClassifier(), param_dist, n_jobs=-1, verbose=10, cv=4, n_iter=200)\n",
    "search.fit(X_train, train[\"ratingCategory\"])\n",
    "print(search.best_score_, search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test[\"description\"])\n",
    "pred = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predictions on test sample\n",
    "submission = pd.DataFrame({'id': test['id'], 'ratingCategory':pred})\n",
    "submission['ratingCategory'] = submission['ratingCategory'].astype('int64')\n",
    "\n",
    "# Make Sure the Category is an Integer\n",
    "submission.head()\n",
    "\n",
    "# Save your Submission File\n",
    "# Best to Use an Integer or Timestamp for different versions of your model\n",
    "submission.to_csv('./data/submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "What you should be doing now:\n",
    "1. Join the Kaggle Competition\n",
    "2. Download the data\n",
    "3. Train a model & try: \n",
    "    - Creating a Text Extraction & Classification Pipeline\n",
    "    - Tune the pipeline with a `GridSearchCV` or `RandomizedSearchCV`\n",
    "    - Add some Latent Semantic Indexing (lsi) into your pipeline. *Note:* You can grid search a nested pipeline, but you have to use double underscores ie `lsi__svd__n_components`\n",
    "    - Try to extract word embeddings with Spacy and use those embeddings as your features for a classification model.\n",
    "4. Make a submission to Kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "To review this module: \n",
    "* Continue working on the Kaggle comeptition\n",
    "* Find another text classification task to work on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
